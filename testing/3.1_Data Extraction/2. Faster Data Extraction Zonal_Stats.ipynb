{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca377d-46d8-4836-9066-c51d7e055c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acee8c1e-a7fe-401e-869c-a5e51955733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import logging\n",
    "from rasterstats import zonal_stats\n",
    "import re\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import gc\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25961094-f965-448b-be53-13eafc8abed1",
   "metadata": {},
   "source": [
    "# File System Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a83c9f-f180-48d9-ad1a-4c359b673d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Function to find the tif files in a given folder¶\n",
    "def find_files_in_folder(folder_path, extension=None, recursive=False):\n",
    "    matched_files = []\n",
    "    # Determine the search pattern based on whether an extension is provided and recursion is enabled\n",
    "    if extension:\n",
    "        if recursive:\n",
    "            search_pattern = os.path.join(folder_path, f\"**/*.{extension}\")\n",
    "        else:\n",
    "            search_pattern = os.path.join(folder_path, f\"*.{extension}\")\n",
    "    else:\n",
    "        # No extension specified, handle both recursive and non-recursive cases\n",
    "        if recursive:\n",
    "            search_pattern = os.path.join(folder_path, \"**/*\")\n",
    "        else:\n",
    "            search_pattern = os.path.join(folder_path, \"*\")\n",
    "    # Use glob to find matching files in the specified directory and subdirectories if recursive\n",
    "    matched_files.extend(glob.glob(search_pattern, recursive=recursive))\n",
    "    # If no files are found, return a list with an empty string\n",
    "    if not matched_files:\n",
    "        matched_files = [\"\"]\n",
    "    return matched_files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2a4694-07cd-4989-9c39-f1917465bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_suffix_to_file_name(file_path, suffix):\n",
    "    \"\"\"\n",
    "    Adds a suffix to the file name in the given file path.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): The original file path.\n",
    "    - suffix (str): The suffix to add to the file name (before the file extension).\n",
    "    \n",
    "    Returns:\n",
    "    - str: The updated file path with the suffix added to the file name.\n",
    "    \"\"\"\n",
    "    # Separate the directory, file name, and extension\n",
    "    directory, file_name = os.path.split(file_path)\n",
    "    base_name, ext = os.path.splitext(file_name)\n",
    "    \n",
    "    # Add the suffix to the file name\n",
    "    new_file_name = f\"{base_name}{suffix}{ext}\"\n",
    "    \n",
    "    # Reconstruct the full file path\n",
    "    new_file_path = os.path.join(directory, new_file_name)\n",
    "    return new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544cdd9-4c54-422c-ad9a-ccadf378a028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6723b20b-0fcd-40fb-a0fa-7c2ec1f28d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def precise_timing_decorator(func):\n",
    "    \"\"\"Decorator for high-precision execution timing.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f\"⏳ Execution time of {func.__name__}: {end_time - start_time:.6f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da17720-897a-4ac9-a4ab-8585bdcf789b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ca0e96-2663-47af-8198-0ca24e3801f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT_STYLES = {\n",
    "    \"default\": {\n",
    "        \"id_fontsize\": 14,\n",
    "        \"id_fontweight\": \"normal\",\n",
    "        \"id_fontcolor\": \"black\",\n",
    "        \"id_fontfamily\": \"sans-serif\",\n",
    "        \"id_bbox\": False  # No bounding box\n",
    "    },\n",
    "    \"bold_high_contrast\": {\n",
    "        \"id_fontsize\": 16,\n",
    "        \"id_fontweight\": \"bold\",\n",
    "        \"id_fontcolor\": \"white\",\n",
    "        \"id_fontfamily\": \"Arial\",\n",
    "        \"id_bbox\": {\"facecolor\": \"black\", \"alpha\": 0.7}  # Dark background for visibility\n",
    "    },\n",
    "    \"large_minimal\": {\n",
    "        \"id_fontsize\": 18,\n",
    "        \"id_fontweight\": \"light\",\n",
    "        \"id_fontcolor\": \"yellow\",\n",
    "        \"id_fontfamily\": \"monospace\",\n",
    "        \"id_bbox\": None  # No bounding box\n",
    "    },\n",
    "    \"compact_subtle\": {\n",
    "        \"id_fontsize\": 12,\n",
    "        \"id_fontweight\": \"medium\",\n",
    "        \"id_fontcolor\": \"darkred\",\n",
    "        \"id_fontfamily\": \"serif\",\n",
    "        \"id_bbox\": {\"facecolor\": \"white\", \"alpha\": 0.5}  # Light background for soft contrast\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a886fc-594e-48b0-b46f-fb4d04256ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@precise_timing_decorator\n",
    "def plot_with_highlighted_ortho_geometry(\n",
    "    shapefile, highlight_id, status, processed_ids, \n",
    "    ortho_path=None, highlight_color='khaki', edgewidth=0.25, \n",
    "    default_color='lightslategrey', opacity=0.4, \n",
    "    plot_size=(5, 5), show_ids=True, \n",
    "    zoom=True, zoom_out_factor=6, \n",
    "    font_style=\"bold_high_contrast\",\n",
    "    id_bbox = False, project_name = \"None\", flight = \"None\"\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots a GeoDataFrame with different colors for various stages of processing.\n",
    "    Optionally overlays the plot on an orthomosaic.\n",
    "    \n",
    "    Parameters:\n",
    "    - shapefile (GeoDataFrame): The input GeoDataFrame containing geometries.\n",
    "    - highlight_id (int or str): The ID of the geometry to highlight.\n",
    "    - status (str): The processing status: 'processing', 'processed', or 'unprocessed'.\n",
    "    - processed_ids (list): List of IDs of already processed polygons.\n",
    "    - ortho_path (str, optional): Path to the orthomosaic GeoTIFF file.\n",
    "    - highlight_color (str): Color for the highlighted geometry's boundary (default: 'red').\n",
    "    - edgewidth (float): Width of the boundary line for the highlighted geometry (default: 3).\n",
    "    - default_color (str): Default color for other geometries (default: 'lightblue').\n",
    "    - opacity (float): Opacity level of the polygons (default: 0.5).\n",
    "    - plot_size (tuple): Size of the plot in inches (default: (10, 10)).\n",
    "    - show_ids (bool): Whether to display the polygon IDs inside the plot (default: True).\n",
    "    - zoom (bool): Whether to zoom in on the highlighted geometry (default: True).\n",
    "    - zoom_out_factor (float): Controls zoom-out level (default: 1.5, where 1 = exact bounds).\n",
    "    - font_style (str): Preset style for ID text. Options:\n",
    "        - \"default\": Standard black text with no background.\n",
    "        - \"bold_high_contrast\": Large, bold white text with a black background.\n",
    "        - \"large_minimal\": Large yellow text with no background.\n",
    "        - \"compact_subtle\": Small, dark red text with a light background.\n",
    "      (default: \"default\")\n",
    "    - id_bbox (bool): Whether to show a white bounding box around ID text (default: False).\n",
    "\n",
    "    \"\"\"\n",
    "    # Clear the current figure\n",
    "    plt.clf()\n",
    "\n",
    "    # Create a new figure with user-defined size\n",
    "    fig, ax = plt.subplots(figsize=plot_size)\n",
    "\n",
    "    # Plot the orthomosaic if provided\n",
    "    if ortho_path:\n",
    "        with rasterio.open(ortho_path) as src:\n",
    "            show(src, ax=ax, title= f\"Raster processing status for {project_name}\", adjust='box')\n",
    "\n",
    "            # Reproject the shapefile to match the orthomosaic CRS if necessary\n",
    "            if shapefile.crs != src.crs:\n",
    "                print(f\"Reprojecting shapefile to match orthomosaic CRS: {src.crs}\")\n",
    "                shapefile = shapefile.to_crs(src.crs)\n",
    "\n",
    "    # Set default zoom boundaries (entire dataset)\n",
    "    minx, miny, maxx, maxy = shapefile.total_bounds  \n",
    "    \n",
    "    # If zoom is enabled, adjust bounds to zoom in on the highlighted geometry\n",
    "    if zoom and highlight_id in shapefile['id'].values:\n",
    "        highlight_geom = shapefile[shapefile['id'] == highlight_id].geometry.iloc[0]\n",
    "        minx, miny, maxx, maxy = highlight_geom.bounds\n",
    "\n",
    "        # Apply zoom out factor (expand bounds)\n",
    "        width = maxx - minx\n",
    "        height = maxy - miny\n",
    "        minx -= width * (zoom_out_factor - 1) / 2\n",
    "        maxx += width * (zoom_out_factor - 1) / 2\n",
    "        miny -= height * (zoom_out_factor - 1) / 2\n",
    "        maxy += height * (zoom_out_factor - 1) / 2\n",
    "\n",
    "        ax.set_xlim(minx, maxx)\n",
    "        ax.set_ylim(miny, maxy)\n",
    "\n",
    "    # Plot each polygon with different colors based on processing status\n",
    "    for idx, geom in shapefile.iterrows():\n",
    "        if geom['id'] in processed_ids:\n",
    "            color = 'green'  # Processed\n",
    "        elif geom['id'] == highlight_id:\n",
    "            color = highlight_color  # Currently processing\n",
    "        else:\n",
    "            color = default_color  # Unprocessed\n",
    "\n",
    "        # Plot the individual geometry with opacity control\n",
    "        shapefile.loc[[idx]].plot(ax=ax, edgecolor='black', facecolor=color, linewidth=edgewidth, alpha=opacity)\n",
    "\n",
    "        # Get font settings from dictionary\n",
    "        font_settings = FONT_STYLES.get(font_style, FONT_STYLES[\"default\"])  # Default if not found\n",
    "    \n",
    "        # Extract font properties\n",
    "        id_fontweight = font_settings[\"id_fontweight\"]\n",
    "        id_fontcolor = font_settings[\"id_fontcolor\"]\n",
    "        id_fontfamily = font_settings[\"id_fontfamily\"]\n",
    "        if id_bbox:\n",
    "            id_bbox = font_settings[\"id_bbox\"]\n",
    "\n",
    "        # Adjust font color based on processing status\n",
    "        if geom['id'] in processed_ids:\n",
    "            id_fontcolor = 'lime'  # Processed\n",
    "        elif geom['id'] == highlight_id:\n",
    "            id_fontcolor = 'yellow'  # Being processed\n",
    "\n",
    "        # Adjust font size based on the zoom level or polygon size\n",
    "        if zoom:\n",
    "            # Calculate font size based on the zoomed area\n",
    "            area = (maxx - minx) * (maxy - miny)  # Area of the zoomed-in region\n",
    "            id_fontsize = area ** 0.5 / 10  # Use square root of area for better scaling\n",
    "            id_fontsize = max(15, min(20, id_fontsize))  # Limit font size to a reasonable range\n",
    "        else:\n",
    "            # Calculate font size based on the polygon size (width or height)\n",
    "            width = maxx - minx\n",
    "            height = maxy - miny\n",
    "            area = width * height\n",
    "            id_fontsize = area ** 0.5 / 10  # Square root of the area for proportionate size\n",
    "            id_fontsize = max(10, min(20, id_fontsize))  # Limit font size to a reasonable range\n",
    "\n",
    "        if show_ids:\n",
    "            centroid = geom.geometry.centroid\n",
    "            # Only print IDs that are within the zoomed/cropped area\n",
    "            if not zoom or (minx <= centroid.x <= maxx and miny <= centroid.y <= maxy):\n",
    "                ax.text(\n",
    "                    centroid.x, centroid.y, str(geom['id']), \n",
    "                    fontsize=id_fontsize, fontweight=id_fontweight, \n",
    "                    color=id_fontcolor, family=id_fontfamily, \n",
    "                    ha='center', va='center',\n",
    "                    bbox=id_bbox if id_bbox else None\n",
    "                    )\n",
    "\n",
    "    # Set the title\n",
    "    ax.set_title(f\"Processing Plot #: {highlight_id}\" + \"\\n\" +  f\"for field {flight}\", fontsize=14)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.xlabel(\"Longitude\", fontsize=12)\n",
    "    plt.ylabel(\"Latitude\", fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc2e2c4-7d87-44d4-9c87-b511c3be4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reduce RAM Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9225cf2c-c991-4d40-938b-6f170d57435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"Clears memory manually.\"\"\"\n",
    "    gc.collect()\n",
    "    print(\"✅ Cleared unused memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0c7a0-6f8e-425d-a71b-c65cad733004",
   "metadata": {},
   "source": [
    "# Getting project files dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6626ca4-41b3-45a0-8f71-76e8c2206d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_files(src_folder, project_name, flight_type='MS'):\n",
    "    \"\"\"\n",
    "    Retrieves orthomosaic and DSM/DTM file paths, organized by flight folder.\n",
    "\n",
    "    Parameters:\n",
    "    - src_folder (str): Root directory where projects are stored.\n",
    "    - project_name (str): Name of the project folder.\n",
    "    - flight_type (str): Flight type, either 'MS' or '3D'.\n",
    "\n",
    "    Returns:\n",
    "    - ortho_dict (dict): { flight_folder_name: [list of orthomosaic files] }\n",
    "    - dsm_dtm_dict (dict): { flight_folder_name: [list of DSM/DTM files] }\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Fetching file lists for project: {project_name}, flight type: {flight_type}\")\n",
    "\n",
    "    # Define base path for the flight type\n",
    "    flight_type_folder = os.path.join(src_folder, project_name, flight_type)\n",
    "\n",
    "    # Find all subfolders within the flight type folder (each representing a flight)\n",
    "    flight_folders = [\n",
    "        f for f in os.listdir(flight_type_folder)\n",
    "        if os.path.isdir(os.path.join(flight_type_folder, f))\n",
    "    ]\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    ortho_dict = {}\n",
    "    dsm_dtm_dict = {}\n",
    "\n",
    "    # Loop through each flight folder\n",
    "    for flight_folder in flight_folders:\n",
    "        flight_path = os.path.join(flight_type_folder, flight_folder)\n",
    "        ortho_folder = os.path.join(flight_path, \"2_Orthomosaics\")\n",
    "        dsm_dtm_folder = os.path.join(flight_path, \"3_DSM_DTM_Elevation_Models\")\n",
    "\n",
    "        # Collect orthomosaic files\n",
    "        if os.path.exists(ortho_folder):\n",
    "            ortho_files = [os.path.join(ortho_folder, f) for f in os.listdir(ortho_folder) if f.endswith(\".tif\")]\n",
    "            if ortho_files:\n",
    "                ortho_dict[flight_folder] = ortho_files\n",
    "        elif 'MS' in flight_folder:\n",
    "            print(f\"Warning: Orthomosaics folder not found in {flight_folder}\")\n",
    "\n",
    "        # Collect DSM/DTM files\n",
    "        if os.path.exists(dsm_dtm_folder):\n",
    "            dsm_dtm_files = [os.path.join(dsm_dtm_folder, f) for f in os.listdir(dsm_dtm_folder) if f.endswith(\".tif\")]\n",
    "            if dsm_dtm_files:\n",
    "                dsm_dtm_dict[flight_folder] = dsm_dtm_files\n",
    "        elif '3D' in flight_folder:\n",
    "            print(f\"Warning: DSM/DTM folder not found in {flight_folder}\")\n",
    "\n",
    "    return ortho_dict, dsm_dtm_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f056dc8-207c-4da9-9fe3-0882b0f909c0",
   "metadata": {},
   "source": [
    "# Ensure data is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84c62c0f-2bf0-4e1d-84b3-c751706cfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "@precise_timing_decorator\n",
    "def check_data_completeness(ortho_dict, dsm_dtm_dict):\n",
    "    \"\"\"\n",
    "    Checks if all required orthomosaic and DSM/DTM files are present for each flight folder,\n",
    "    ensuring consistency in flight types (either \"MS\" or \"3D\").\n",
    "\n",
    "    Parameters:\n",
    "    - ortho_dict (dict): Dictionary {flight_folder: list_of_orthomosaics}.\n",
    "    - dsm_dtm_dict (dict): Dictionary {flight_folder: list_of_dsm_dtm_files}.\n",
    "\n",
    "    Returns:\n",
    "    - completeness_status (dict): Summary of completeness for each flight.\n",
    "    - missing_files_report (dict): Flights with missing data and missing filenames.\n",
    "    \"\"\"\n",
    "\n",
    "    completeness_status = {}\n",
    "    missing_files_report = {}\n",
    "\n",
    "    # Define required file suffixes for MS and 3D projects\n",
    "    required_files_ms = {\n",
    "        \"_index_green_green.tif\",\n",
    "        \"_index_ndvi.tif\",\n",
    "        \"_index_nir_nir.tif\",\n",
    "        \"_index_red_edge_red_edge.tif\",\n",
    "        \"_index_red_red.tif\",\n",
    "        \"_transparent_mosaic_group1.tif\"\n",
    "    }\n",
    "    optional_blue_band = \"_index_blue_blue.tif\"\n",
    "\n",
    "    # # For older datasets. Remove Later\n",
    "    # # Define required file suffixes for MS and 3D projects\n",
    "    # required_files_ms = {\n",
    "    #     \"_transparent_reflectance_green.tif\",\n",
    "    #     \"_index_ndvi.tif\",\n",
    "    #     \"_transparent_reflectance_nir.tif\",\n",
    "    #     \"_transparent_reflectance_red_edge.tif\",\n",
    "    #     \"_transparent_reflectance_red.tif\",\n",
    "    # }\n",
    "    # optional_blue_band = \"_transparent_reflectance_blue.tif\"\n",
    "    \n",
    "\n",
    "    required_files_3d = {\n",
    "        \"_dtm.tif\",\n",
    "        \"_dsm.tif\"\n",
    "    }\n",
    "\n",
    "    # Identify flight types from folder names\n",
    "    flight_types = {}\n",
    "    for flight in list(ortho_dict.keys()) + list(dsm_dtm_dict.keys()):\n",
    "        if \"MS\" in flight.upper():\n",
    "            flight_types[flight] = \"MS\"\n",
    "        elif \"3D\" in flight.upper():\n",
    "            flight_types[flight] = \"3D\"\n",
    "        else:\n",
    "            flight_types[flight] = \"UNKNOWN\"\n",
    "\n",
    "    # Count occurrences of each flight type\n",
    "    type_counts = Counter(flight_types.values())\n",
    "\n",
    "    # If there are mixed flight types, identify the minority group and warn the user\n",
    "    if len(type_counts) > 1:\n",
    "        print(\"\\n⚠️ WARNING: Inconsistent flight types detected!\")\n",
    "        print(\"Detected flight types:\", type_counts)\n",
    "\n",
    "        # Find the minority group (least frequent flight type)\n",
    "        minority_type = min(type_counts, key=type_counts.get)\n",
    "        minority_flights = [flight for flight, f_type in flight_types.items() if f_type == minority_type]\n",
    "\n",
    "        print(f\"🚨 The following flights are of type '{minority_type}', which may be misplaced:\")\n",
    "        print(\"\\n\".join(minority_flights))\n",
    "        print(\"Please verify if these are in the correct location.\\n\")\n",
    "\n",
    "    # Perform completeness checks\n",
    "    for flight, f_type in flight_types.items():\n",
    "        if f_type == \"MS\":\n",
    "            # Check for missing MS files\n",
    "            files = ortho_dict.get(flight, [])\n",
    "            missing_files = [f for f in required_files_ms if not any(f in file for file in files)]\n",
    "\n",
    "            # Check for blue band consistency\n",
    "            blue_band_found = any(optional_blue_band in file for file in files)\n",
    "\n",
    "            if not missing_files:\n",
    "                completeness_status[flight] = \"✅ Complete\"\n",
    "            else:\n",
    "                completeness_status[flight] = \"❌ Incomplete\"\n",
    "                missing_files_report[flight] = missing_files\n",
    "\n",
    "        elif f_type == \"3D\":\n",
    "            # Check for missing 3D files\n",
    "            files = dsm_dtm_dict.get(flight, [])\n",
    "            missing_files = [f for f in required_files_3d if not any(f in file for file in files)]\n",
    "\n",
    "            if not missing_files:\n",
    "                completeness_status[flight] = \"✅ Complete\"\n",
    "            else:\n",
    "                completeness_status[flight] = \"❌ Incomplete\"\n",
    "                missing_files_report[flight] = missing_files\n",
    "\n",
    "        else:\n",
    "            print(f\"⚠️ Unknown flight type for {flight}. Skipping data check.\")\n",
    "\n",
    "    return completeness_status, missing_files_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303358d-8a8f-4541-9369-52205c92b29b",
   "metadata": {},
   "source": [
    "## Drop incomplete projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feda10fb-a56a-42e4-b2aa-51fbb1bab6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def drop_incomplete_flights(ortho_dict, dsm_dtm_dict, missing, output_csv_folder):\n",
    "    \"\"\"\n",
    "    Drops flights with missing files from the ortho_dict and dsm_dtm_dict.\n",
    "    \n",
    "    Args:\n",
    "        ortho_dict (dict): Dictionary mapping flight names to orthomosaic file paths.\n",
    "        dsm_dtm_dict (dict): Dictionary mapping flight names to DSM and DTM file paths.\n",
    "        missing (dict): Dictionary listing missing files for each flight.\n",
    "    \n",
    "    Returns:\n",
    "        ortho_dict (dict): Updated ortho_dict with incomplete flights removed.\n",
    "        dsm_dtm_dict (dict): Updated dsm_dtm_dict with incomplete flights removed.\n",
    "    \"\"\"\n",
    "    for flight in list(missing.keys()):  # Use list() to avoid modifying dict during iteration\n",
    "        if flight in ortho_dict:\n",
    "            del ortho_dict[flight]\n",
    "        if flight in dsm_dtm_dict:\n",
    "            del dsm_dtm_dict[flight]\n",
    "\n",
    "    # write_missing_projects to csv\n",
    "    csv_log = os.path.join(output_csv_folder, \"Incomplete Flights Log.csv\")\n",
    "    write_missing_projects_to_csv(missing, csv_log)\n",
    "\n",
    "    return ortho_dict, dsm_dtm_dict\n",
    "\n",
    "def write_missing_projects_to_csv(missing_projects, csv_file):\n",
    "    \"\"\"\n",
    "    Write the missing projects (with missing files) to a CSV file.\n",
    "    Each missing file will be logged as a separate row along with the project ID and a note.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    output_dir = os.path.dirname(csv_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    \n",
    "    # Check if the CSV file already exists\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    \n",
    "    # Open the CSV file in append mode\n",
    "    with open(csv_file, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'missing_file', 'status'])\n",
    "        \n",
    "        # Write the header if the file does not exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write each missing project and its corresponding missing files to the CSV\n",
    "        for project_id, missing_files in missing_projects.items():\n",
    "            for missing_file in missing_files:\n",
    "                writer.writerow({\n",
    "                    'id': project_id,\n",
    "                    'missing_file': missing_file,\n",
    "                    'status': 'Incomplete'  # You can change this if you want a different status\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b23c62-089e-4f56-beb2-dcf125067bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9290c988-16a0-49b4-8f87-9c005d87b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_common_prefix(file_paths):\n",
    "    \"\"\"Finds the common prefix among a list of file names.\"\"\"\n",
    "    common_prefix = os.path.commonprefix([os.path.basename(p) for p in file_paths])\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '', common_prefix.replace(\" \", \"_\"))  # Clean any special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a29c97-bc60-4f9a-b603-d278ebb94ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e14a6e-a1e8-466a-b6fa-954fac17ab72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b90d9a-63d6-4ca8-a772-2bdd1fe475df",
   "metadata": {},
   "source": [
    "# Downscale tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a80eecb3-af7d-4d1d-bbc2-e56b65405437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.enums import Resampling\n",
    "def downscale_image(file_path, output_folder, scale_factor):\n",
    "    \"\"\"\n",
    "    Downscale a TIFF/JPG file and save it in the structured output directory.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the original file.\n",
    "        output_folder (str): The base output folder.\n",
    "        scale_factor (float): Scaling factor (e.g., 0.25 for 25%).\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved downscaled file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n⚙️ Processing: {os.path.basename(file_path)} at {int(scale_factor * 100)}% resolution\")\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Compute new dimensions\n",
    "            new_width = int(src.width * scale_factor)\n",
    "            new_height = int(src.height * scale_factor)\n",
    "            print(f\"   - Original size: {src.width} x {src.height}\")\n",
    "            print(f\"   - New size: {new_width} x {new_height}\")\n",
    "\n",
    "            # Read and resample image\n",
    "            resampled_array = src.read(\n",
    "                out_shape=(src.count, new_height, new_width),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "\n",
    "            # Update metadata\n",
    "            profile = src.profile\n",
    "            profile.update(\n",
    "                width=new_width,\n",
    "                height=new_height,\n",
    "                transform=src.transform * src.transform.scale(\n",
    "                    (src.width / new_width), (src.height / new_height)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "            # Save downscaled image\n",
    "            print(f\"   - Saving downscaled file: {output_folder}\")\n",
    "            with rasterio.open(output_folder, \"w\", **profile) as dst:\n",
    "                dst.write(resampled_array)\n",
    "\n",
    "            print(f\"✅ Successfully downscaled: {os.path.basename(file_path)}\")\n",
    "            return output_folder\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR processing {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef19e7-0af0-4b75-b9a5-2a6ce4d3994d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "697754d5-9b5d-46c6-acd9-2600adb0bf39",
   "metadata": {},
   "source": [
    "# Data Extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d6abdf2-eacf-4ba8-b0ba-b89f898cfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK WHAT DOES IT DO WITH NDVI TIF if given in input rasters\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Column order as specified\n",
    "STAT_ORDER = [\n",
    "    \"count\", \"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\", \"range\", \"minority\", \"majority\", \"variety\", \n",
    "    \"variance\", \"cv\", \"skewness\", \"kurtosis\", \"top_10\", \"top_15\", \"top_20\", \"top_25\", \"top_35\", \"top_50\", \n",
    "    \"q25\", \"q75\", \"iqr\"\n",
    "]\n",
    "MORE_STAT_PERCENT = [5, 10, 15, 25, 35, 50]\n",
    "\n",
    "# Raster sorting order\n",
    "RASTER_ORDER_ = [\"blue\", \"green\", \"ndvi\", \"nir\", \"red\", \"rededge\"]\n",
    "RASTER_NAME_KEYS_ = [\"blue_blue\", \"green_green\", \"nir_nir\", \"red_red\", \"red_edge_red_edge\", \"ndvi_ndvi\"]\n",
    "# RASTER_NAME_KEYS_ = [\"reflectance_blue\", \"reflectance_green\", \"reflectance_nir\", \"reflectance_red\", \"reflectance_red_edge\", \"reflectance_ndvi\"]\n",
    "\n",
    "def match_raster_keys(order_list, name_keys):\n",
    "    \"\"\"\n",
    "    Maps raster order names to their corresponding file name keys by sorting the name_keys and normalizing both lists.\n",
    "    Parameters:\n",
    "        order_list (list): List of raster order names.\n",
    "        name_keys (list): List of raster file name keys.\n",
    "    Returns:\n",
    "        dict: Dictionary mapping order names to matching file name keys.\n",
    "    \"\"\"\n",
    "    sorted_keys = sorted(name_keys, key=len, reverse=False)  # Sort longest names first\n",
    "\n",
    "    def normalize(name):\n",
    "        return re.sub(r'[^a-zA-Z0-9]', '', name.lower())  # Remove special characters\n",
    "\n",
    "    normalized_keys = {normalize(key): key for key in sorted_keys}  # Map normalized names to original names\n",
    "    mapping = {}\n",
    "\n",
    "    for order in order_list:\n",
    "        norm_order = normalize(order)\n",
    "        for norm_key, original_key in normalized_keys.items():\n",
    "            if norm_order in norm_key:  # Exact match after stripping characters\n",
    "                mapping[order] = original_key\n",
    "                break\n",
    "    return mapping\n",
    "\n",
    "RASTER_ORDER = match_raster_keys(RASTER_ORDER_, RASTER_NAME_KEYS_)\n",
    "\n",
    "def ensure_crs_alignment(gdf, raster_crs):\n",
    "    \"\"\"Ensures the GeoDataFrame has the same CRS as the raster.\"\"\"\n",
    "    try:\n",
    "        if gdf.crs != raster_crs:\n",
    "            logging.warning(\"📌 CRS mismatch detected. Reprojecting shapefile to match raster CRS.\")\n",
    "            print(\"📌 CRS mismatch detected. Reprojecting shapefile to match raster CRS.\")\n",
    "            print(f\"📌 GEOJSON CRS {gdf.crs} to Raster CRS {raster_crs}.\")\n",
    "\n",
    "            gdf = gdf.to_crs(raster_crs)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while aligning CRS: {e}\")\n",
    "        raise\n",
    "    return gdf\n",
    "\n",
    "def find_raster_files(paths, RASTER_ORDER):\n",
    "    \"\"\"Identifies raster files based on naming conventions.\"\"\"\n",
    "    rasters = {name: next((tif for tif in paths if name_pattern in tif), None) for name, name_pattern in RASTER_ORDER.items()}\n",
    "    return {k: v for k, v in rasters.items() if v is not None}\n",
    "\n",
    "# Ensure this function remains unchanged\n",
    "# Optimized NDVI Computation Using Numba\n",
    "@njit\n",
    "def compute_ndvi_numba(nir, red):\n",
    "    \"\"\"Fast NDVI computation using Numba.\"\"\"\n",
    "    return np.where((nir + red) == 0, np.nan, (nir - red) / (nir + red))\n",
    "\n",
    "def generate_ndvi_GeoTIFF_and_values(nir_tif, red_tif, output_tif):\n",
    "    \"\"\"Generates an NDVI orthomosaic and calculates values.\"\"\"\n",
    "    with rasterio.open(nir_tif) as nir_src, rasterio.open(red_tif) as red_src:\n",
    "        # Read the data\n",
    "        nir = nir_src.read(1).astype(np.float32)\n",
    "        red = red_src.read(1).astype(np.float32)\n",
    "        transform = nir_src.transform\n",
    "        crs = nir_src.crs\n",
    "\n",
    "        # Compute NDVI\n",
    "        ndvi = compute_ndvi_numba(nir, red)\n",
    "\n",
    "        # Save NDVI to a GeoTIFF\n",
    "        profile = nir_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1, compress=\"lzw\")\n",
    "\n",
    "        with rasterio.open(output_tif, \"w\", **profile) as dst:\n",
    "            dst.write(ndvi, 1)\n",
    "\n",
    "    # Compute NDVI statistics (flatten non-NaN values)\n",
    "    ndvi_values = ndvi[~np.isnan(ndvi)].flatten()\n",
    "\n",
    "    return output_tif, ndvi\n",
    "\n",
    "def compute_additional_statistics(values, MORE_STAT_PERCENT):\n",
    "    \"\"\"Computes variance, CV, skewness, kurtosis, variety, and percentile-based stats.\n",
    "        Parameters:\n",
    "        values (array-like): List or array of raster values.\n",
    "        MORE_STAT_PERCENT (list) : List of additional stats percentages\n",
    "    Returns:\n",
    "        dict: Dictionary of computed statistics.\n",
    "    \"\"\"\n",
    "    percent_stats = list(itertools.chain.from_iterable([[f\"top_{percent}_mean\", f\"top_{percent}_median\", f\"top_{percent}_std\"] for percent in MORE_STAT_PERCENT]))\n",
    "\n",
    "    try:\n",
    "        if len(values) == 0:\n",
    "            return {stat: np.nan for stat in STAT_ORDER + percent_stats if stat not in STAT_ORDER[:10]}\n",
    "\n",
    "        values = np.asarray(values).flatten()  # Ensure it's a NumPy array\n",
    "\n",
    "        stats = {\n",
    "            \"variety\": len(np.unique(values)),\n",
    "            \"variance\": np.nanvar(values),\n",
    "            \"cv\": np.nanstd(values) / np.nanmean(values) if np.nanmean(values) != 0 else np.nan,\n",
    "            \"skewness\": pd.Series(values).skew(),\n",
    "            \"kurtosis\": pd.Series(values).kurtosis(),\n",
    "            \"top_10\": np.nanpercentile(values, 90),\n",
    "            \"top_15\": np.nanpercentile(values, 85),\n",
    "            \"top_20\": np.nanpercentile(values, 80),\n",
    "            \"top_25\": np.nanpercentile(values, 75),\n",
    "            \"top_35\": np.nanpercentile(values, 65),\n",
    "            \"top_50\": np.nanpercentile(values, 50),\n",
    "            \"q25\": np.nanpercentile(values, 25),\n",
    "            \"q75\": np.nanpercentile(values, 75),\n",
    "            \"iqr\": np.nanpercentile(values, 75) - np.nanpercentile(values, 25)\n",
    "            }\n",
    "\n",
    "        # Compute statistics on top X% of the raw data\n",
    "        # values_ = np.concatenate(values)\n",
    "        values_ = values\n",
    "        for percent in MORE_STAT_PERCENT:\n",
    "            threshold = np.percentile(values_, 100 - percent)\n",
    "            top_values = values_[values_ >= threshold]\n",
    "\n",
    "            if len(top_values) > 0:\n",
    "                stats.update({f\"top_{percent}_mean\": np.mean(top_values)})\n",
    "                stats.update({f\"top_{percent}_median\": np.median(top_values)})\n",
    "                stats.update({f\"top_{percent}_std\": np.std(top_values)})\n",
    "            else:\n",
    "                stats[f\"top_{percent}_mean\"] = np.nan\n",
    "                stats[f\"top_{percent}_median\"] = np.nan\n",
    "                stats[f\"top_{percent}_std\"] = np.nan\n",
    "\n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error computing additional statistics: {e}\")\n",
    "        return {stat: np.nan for stat in STAT_ORDER + percent_stats if stat not in STAT_ORDER[:10]}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def process_raster_statistics(stats, gdf, raster_name, project_name, flight, more_stat_percent):\n",
    "    \"\"\"\n",
    "    Processes raster statistics by extracting raw values, computing additional statistics,\n",
    "    and formatting the final DataFrame with necessary metadata.\n",
    "\n",
    "    Parameters:\n",
    "        stats (list of dict): List of dictionaries containing raster statistics.\n",
    "        gdf (GeoDataFrame): Geospatial dataframe containing feature IDs.\n",
    "        raster_name (str): Name of the raster being processed.\n",
    "        project_name (str): Project identifier.\n",
    "        flight (str): Flight identifier.\n",
    "        more_stat_percent (list): List of percent values for additional statistics.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed statistics DataFrame.\n",
    "    \"\"\"\n",
    "    # Extract raw values for additional calculations\n",
    "    raw_values = [\n",
    "        stat[\"mini_raster_array\"].compressed() if \"mini_raster_array\" in stat else np.array([])\n",
    "        for stat in stats\n",
    "    ]\n",
    "\n",
    "    additional_stats = [compute_additional_statistics(vals, more_stat_percent) for vals in raw_values]\n",
    "\n",
    "    # Create DataFrame from stats and drop unwanted columns\n",
    "    df_stats = pd.DataFrame(stats).drop(columns=[\"mini_raster_array\", \"mini_raster_affine\", \"mini_raster_nodata\"], errors=\"ignore\")\n",
    "    df_stats = df_stats[STAT_ORDER[:10]]  # Assign expected column order\n",
    "\n",
    "    # Convert additional statistics into DataFrame\n",
    "    df_additional = pd.DataFrame(additional_stats)\n",
    "\n",
    "    # Merge basic and additional statistics\n",
    "    df = pd.concat([df_stats, df_additional], axis=1)\n",
    "\n",
    "    # Prefix raster name to stat columns\n",
    "    df.columns = [f\"{raster_name}_{col}\" for col in df.columns]\n",
    "\n",
    "    # Insert metadata columns\n",
    "    df.insert(0, \"id\", gdf.id)\n",
    "    df.insert(1, \"project\", project_name)\n",
    "    df.insert(2, \"flight\", flight)\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_raster_stats_multiple(shp, raster_paths, project_name, flight, output_root_folder_path, ndvi_geotiff= True, plot_geom=\"none\"):\n",
    "    \"\"\"\n",
    "    Extracts raster statistics for multiple raster files and saves results in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        shp (str): Path to GeoJSON or shapefile containing geometries.\n",
    "        raster_paths (dict): Dictionary of raster names and their file paths.\n",
    "        project_name (str): Project name.\n",
    "        flight (str): Flight name.\n",
    "        output_root_folder_path (str)\n",
    "        plot_geom (str, optional): Plot geometry option. Default is \"none\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with extracted raster statistics for each raster.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(raster_paths)\n",
    "\n",
    "    raster_example = raster_paths['nir']\n",
    "    output_folder = os.path.join(output_root_folder_path, project_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Generating NDVI GEOTIFF and values\n",
    "    output_ndvi = os.path.join(output_folder, f\"{flight}ndvi.tif\")\n",
    "    # **Check if NDVI GEOTIFF for the flight being processed already exists**\n",
    "\n",
    "    logging.info(f\"Processing project: {project_name}, Flight: {flight}\")\n",
    "    logging.info(f\"⚙️ Generating NDVI Orthomosaic for: {project_name}, Flight: {flight}\")\n",
    "\n",
    "    # Generating NDVI GEOTIFF\n",
    "    if 'nir' and 'red' in raster_paths.keys():\n",
    "        _, ndvi_values = generate_ndvi_GeoTIFF_and_values(raster_paths['nir'], raster_paths['red'], output_ndvi)\n",
    "        logging.info(f\"✅ Successfully generated NDVI GEOTIFF\")\n",
    "\n",
    "    else:\n",
    "        logging.error(f\"❌ Required rasters not found for NDVI GEOTIFF generation. NIR & Red rasters required.\")\n",
    "\n",
    "    logging.info(f\"🚀 Preparing raster processing for flight: {flight}\")\n",
    "    logging.info(f\"⚙️ Reading shapefile: {os.path.basename(shp)}\")\n",
    "\n",
    "    # Importing geojson file\n",
    "    if not os.path.exists(shp):\n",
    "        logging.error(f\"❌ Shapefile not found: {shp}\")\n",
    "        raise FileNotFoundError(f\"Shapefile not found: {shp}\")\n",
    "\n",
    "    try:\n",
    "        gdf = gpd.read_file(shp)\n",
    "        if gdf.empty:\n",
    "            logging.error(\"❌ Shapefile is empty!\")\n",
    "            raise ValueError(\"Shapefile contains no geometries.\")\n",
    "        gdf.columns = gdf.columns.str.lower()  # Convert column names to lowercase\n",
    "\n",
    "        # Ensure 'id' is numeric and sort geodataframe on 'id' column\n",
    "        if pd.to_numeric(gdf[\"id\"], errors=\"coerce\").notna().all():\n",
    "            gdf = gdf.sort_values(by=\"id\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # Ensure CRS alignment between gdf and one of the rastesrs\n",
    "        with rasterio.open(raster_example) as src:\n",
    "            gdf = ensure_crs_alignment(gdf, src.crs)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading shapefile: {e}\")\n",
    "        raise\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # Compute NDVI stats\n",
    "    # zonal_stats requires spatial information to correctly match geometries with raster values.\n",
    "    # When using a file path, it reads the affine transformation automatically.\n",
    "    # When passing a NumPy array, we must manually specify the affine transformation.\n",
    "    logging.info(f\"⚙️ Processing NDVI Stats\")\n",
    "\n",
    "    try:\n",
    "        # We can use original affine from one of the rasters\n",
    "        with rasterio.open(raster_example) as src:\n",
    "            original_affine = src.transform\n",
    "        stats = zonal_stats(\n",
    "            gdf, ndvi_values,\n",
    "            affine=original_affine,\n",
    "            stats=STAT_ORDER[:10],  # Extracting basic stats\n",
    "            raster_out=True  # Extract raw pixel values for additional stats\n",
    "        )\n",
    "\n",
    "        processed_df = process_raster_statistics(stats, gdf, 'NDVI', project_name, flight, MORE_STAT_PERCENT)\n",
    "        all_results.append(processed_df)\n",
    "        logging.info(f\"✅ Successfully processed: NDVI Stats\")\n",
    "    except:\n",
    "        logging.error(f\"❌ Error generating NDVI Statictics.\")\n",
    "\n",
    "    # Compute stats for rasters\n",
    "    for raster_name in raster_paths.keys():\n",
    "        if raster_name not in raster_paths:\n",
    "            continue\n",
    "        raster_path = raster_paths[raster_name]\n",
    "        logging.info(f\"⚙️ Processing raster: {raster_name} ({os.path.basename(raster_path)})\")\n",
    "\n",
    "        # Validate raster file existence\n",
    "        if not os.path.exists(raster_path):\n",
    "            logging.error(f\"Raster file not found: {raster_path}\")\n",
    "            continue  # Skip missing rasters instead of stopping execution\n",
    "\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            gdf = ensure_crs_alignment(gdf, src.crs)\n",
    "\n",
    "            stats = zonal_stats(\n",
    "                gdf, raster_path,\n",
    "                stats=STAT_ORDER[:10],  # Extracting basic stats\n",
    "                raster_out=True  # Extract raw pixel values for additional stats\n",
    "            )\n",
    "        processed_df = process_raster_statistics(stats, gdf, raster_name, project_name, flight, MORE_STAT_PERCENT)\n",
    "\n",
    "        all_results.append(processed_df)\n",
    "        logging.info(f\"✅ Successfully processed: {raster_name}\")\n",
    "\n",
    "    # Merge all results\n",
    "    if not all_results:\n",
    "        # logging.error(\"❌ No raster statistics could be extracted.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(all_results, axis=1).loc[:, ~pd.concat(all_results, axis=1).columns.duplicated()]\n",
    "\n",
    "    percent_stats = list(itertools.chain.from_iterable([[f\"top_{percent}_mean\", f\"top_{percent}_median\", f\"top_{percent}_std\"] for percent in MORE_STAT_PERCENT]))\n",
    "\n",
    "\n",
    "    output_file = os.path.join(output_folder, f\"{flight}statistics.csv\")\n",
    "\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "\n",
    "    logging.info(f\"✅📌 Statistics saved to: {output_file}\")\n",
    "    print(f\"✅📌 Statistics saved to: {output_file}\")\n",
    "    print(f\"⏳ Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bcc01e-c48a-47d7-8dc0-88373ff21a72",
   "metadata": {},
   "source": [
    "# Example use for singel project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd92cc0e-7245-404e-9f38-8b97e266178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Usage\n",
    "# src_folder = r'D:\\PhenoCrop\\2_pix4d_cleaned\\E166\\MS\\20240812 E166 M3M 30m MS 80 85\\2_Orthomosaics'\n",
    "# paths = find_files_in_folder(src_folder, 'tif')\n",
    "# raster_files = find_raster_files(paths, RASTER_ORDER)\n",
    "# # shapefile_path = r\"D:\\PhenoCrop\\3_qgis\\3_Extraction Polygons\\3. FINAL MASKS PYTHON\\24 E 166_test.geojson\"\n",
    "# shapefile_path = r\"D:\\PhenoCrop\\3_qgis\\3_Extraction Polygons\\3. FINAL MASKS PYTHON\\24 E166_sorted_ID_corrected_coordinate_system_polygons_shrinked.geojson\"\n",
    "# project = \"E166\"\n",
    "# flight = \"20240812 E166 M3M 30m MS 80 85\"\n",
    "# output_folder = r\"D:\\PhenoCrop\\3_python\\test\"\n",
    "\n",
    "# stats_df = extract_raster_stats_multiple(shapefile_path, raster_files, project, flight, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f66f6f-613d-45ca-8c7a-b09b2491ad12",
   "metadata": {},
   "source": [
    "# Processing multiple fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "611bf846-b849-4355-af49-922d33da3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import gc\n",
    "import psutil  # To monitor memory usage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from shapely.geometry import box\n",
    "\n",
    "def extract_common_prefix(file_paths):\n",
    "    \"\"\"Finds the common prefix among a list of file names.\"\"\"\n",
    "    common_prefix = os.path.commonprefix([os.path.basename(p) for p in file_paths])\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '', common_prefix.replace(\" \", \"_\"))  # Clean any special characters\n",
    "\n",
    "def list_geojson_files(geojson_file_folder):\n",
    "    \"\"\"\n",
    "    Lists all geojson files in the specified folder and extracts project names from their filenames.\n",
    "\n",
    "    Parameters:\n",
    "    - geojson_file_folder (str): Path to the folder containing geojsonfiles.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary mapping project names to their corresponding geojson file paths.\n",
    "    \"\"\"\n",
    "    geojson_file_dict = {}\n",
    "    geojson_files = glob.glob(os.path.join(geojson_file_folder, \"*.geojson\"))\n",
    "\n",
    "    for shp in geojson_files:\n",
    "        project_name = os.path.basename(shp).split(\".geojson\")[0]\n",
    "        geojson_file_dict[project_name] = shp  # Map project name to geojsonfile path\n",
    "\n",
    "    return geojson_file_dict\n",
    "\n",
    "def check_memory_usage(threshold=80):\n",
    "    \"\"\"\n",
    "    Checks memory usage and prints a warning if it exceeds the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - threshold (int): Memory usage percentage above which a warning is triggered.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if memory usage is high, False otherwise.\n",
    "    \"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    if mem.percent > threshold:\n",
    "        print(f\"⚠️ WARNING: High memory usage detected ({mem.percent}%). Consider freeing memory.\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@precise_timing_decorator\n",
    "def prepare_and_run_raster_processing(project_name, output_root_folder, ortho_dict, geojson_file_folder):\n",
    "    \"\"\"\n",
    "    Matches orthomosaics to the correct geojson file and prepares necessary data before running process_rasters().\n",
    "\n",
    "    Parameters:\n",
    "    - project_name (str): Name of the field project\n",
    "    - output_root_folder (str): Root path to store output CSV files.\n",
    "    - ortho_dict (dict): Dictionary mapping flight folders to lists of raster file paths.\n",
    "    - geojson_file_folder (str): Path to the folder containing geojsonfiles.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    geojson_file_dict = list_geojson_files(geojson_file_folder)\n",
    "\n",
    "    for flight, paths in ortho_dict.items():\n",
    "        print(f\"\\n🚀 Preparing raster processing for flight: {flight}\")\n",
    "\n",
    "        # Find the corresponding geojsonfile\n",
    "        matched_geojson_file = next(\n",
    "            (shp for shp_file_name, shp in geojson_file_dict.items() if project_name in shp_file_name), None\n",
    "        )\n",
    "\n",
    "        if not matched_geojson_file:\n",
    "            print(f\"⚠️ No matching geojson file found for {flight}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"✅ Matched geojson file: {matched_geojson_file}\")\n",
    "\n",
    "        \n",
    "        # Define output file name based on the common prefix of input files\n",
    "        output_csv_name = f\"{extract_common_prefix(paths)}statistics.csv\"\n",
    "        output_csv_path = os.path.join(output_root_folder, project_name, output_csv_name)\n",
    "        \n",
    "        # Ensure the the output directory exists\n",
    "        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "        \n",
    "        # **Check if CSV file for the flight being processed already exists**\n",
    "        if os.path.exists(output_csv_path):\n",
    "            print(f\"📌 Output CSV already exists: {output_csv_path}. Skipping processing.\")\n",
    "            continue  # Skip this flight\n",
    "        \n",
    "        # **Monitor memory before loading large files**\n",
    "        check_memory_usage()\n",
    "\n",
    "        # Identify rgb raster path dynamically\n",
    "        rgb_raster_path = next((tif for tif in paths if \"_transparent_mosaic_group1.tif\" in tif), None)\n",
    "\n",
    "        # Downscale the RGB raster for preview during processing\n",
    "        if rgb_raster_path:\n",
    "            rgb_raster_thumb_path = add_suffix_to_file_name(rgb_raster_path, \"_thumb\")\n",
    "            if os.path.exists(rgb_raster_thumb_path):\n",
    "                print(f\"📌 Resized RGB Ortho exists: {rgb_raster_thumb_path}. Skipping downscaling.\")\n",
    "            else:\n",
    "                downscale_image(rgb_raster_path, rgb_raster_thumb_path, 0.25)\n",
    "\n",
    "        # Generate dick of raster file paths\n",
    "        raster_files = find_raster_files(paths, RASTER_ORDER)\n",
    "\n",
    "        # **Check memory usage after loading**\n",
    "        check_memory_usage()\n",
    "\n",
    "        stats_df = extract_raster_stats_multiple(matched_geojson_file, raster_files, project_name, extract_common_prefix(paths), output_root_folder)\n",
    "\n",
    "\n",
    "        # plot_geom = \"ortho\"\n",
    "        # plot_geom = \"shp\"\n",
    "        # plot_geom = \"none\"\n",
    "\n",
    "        # **Clear raster data from memory**\n",
    "        gc.collect()  # Force garbage collection\n",
    "        print(f\"🧹 Cleared raster data from memory.\")\n",
    "\n",
    "        gc.collect()  # Force garbage collection\n",
    "        print(f\"🧹 Cleared shapefile data from memory.\")\n",
    "\n",
    "        # Save results\n",
    "        # stats_df.to_csv(output_csv_path, index_label=\"id\")\n",
    "        print(f\"📁 Saved results to {output_csv_path}\")\n",
    "\n",
    "        # **Final memory check**\n",
    "        check_memory_usage()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     geojson_file_folder = r'D:\\PhenoCrop\\3_qgis\\3_Extraction Polygons\\3. FINAL MASKS PYTHON'\n",
    "#     output_root_folder = r'D:\\PhenoCrop\\3_python'\n",
    "#     prepare_and_run_raster_processing(project_name, output_root_folder, ortho_dict, geojson_file_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "01d61883-6d80-4197-96cd-95a0ffb34e1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\PhenoCrop\\\\2_pix4d_cleaned'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m field_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE166\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_BAR_VOLL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOAT_FRONTIERS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDIVERSITY_OATS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_BAR_SØRÅS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPHENO_CROP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILOT\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     45\u001b[0m src_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPhenoCrop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2_pix4d_cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 46\u001b[0m project_names \u001b[38;5;241m=\u001b[39m [field \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m field_ids \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(src_folder)]   \u001b[38;5;66;03m# List of projects\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# project_names_TEST = ['E166']   # TEMP List of projects\u001b[39;00m\n\u001b[0;32m     48\u001b[0m geojson_file_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPhenoCrop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m3_qgis\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m3_Extraction Polygons\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m3. FINAL MASKS PYTHON\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\PhenoCrop\\\\2_pix4d_cleaned'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_multiple_projects(project_names, src_folder, flight_type, geojson_file_folder, output_root_folder):\n",
    "    \"\"\"\n",
    "    Processes multiple projects by fetching orthomosaic files, validating data, and running raster analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - project_names (list of str): List of project names to process.\n",
    "    - src_folder (str): Root folder containing project data.\n",
    "    - flight_type (str): Either 'MS' or '3D' indicating the flight type.\n",
    "    - geojson_file_folder (str): Path to folder containing GeoJSON shapefiles.\n",
    "    - output_root_folder (str): Folder where processed CSV files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for project_name in project_names:\n",
    "        print(f\"\\n🚀 Processing Project: {project_name} | Flight Type: {flight_type}\")\n",
    "\n",
    "        # **Step 1: Generate ortho_dict**\n",
    "        ortho_dict, dsm_dtm_dict = get_project_files(src_folder, project_name, flight_type)\n",
    "\n",
    "\n",
    "\n",
    "        completeness, missing = check_data_completeness(ortho_dict, dsm_dtm_dict)\n",
    "\n",
    "        print(\"\\n✅ COMPLETENESS STATUS:\")\n",
    "        for flight, status in completeness.items():\n",
    "            print(f\"{flight}: {status}\")\n",
    "\n",
    "        if missing:\n",
    "            print(\"\\n❌ MISSING FILES REPORT:\")\n",
    "            for flight, files in missing.items():\n",
    "                print(f\"{flight} is missing: {', '.join(files)}\")\n",
    "\n",
    "        # Drop incomplete flights\n",
    "        ortho_dict, dsm_dtm_dict = drop_incomplete_flights(ortho_dict, dsm_dtm_dict, missing, output_root_folder)\n",
    "\n",
    "        # **Step 4: Run raster processing**\n",
    "        prepare_and_run_raster_processing(project_name, output_root_folder, ortho_dict, geojson_file_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    field_ids = ['E166', 'PRO_BAR_VOLL', 'OAT_FRONTIERS', 'DIVERSITY_OATS', 'PRO_BAR_SØRÅS', 'PHENO_CROP', 'PILOT']\n",
    "    src_folder = r\"D:\\PhenoCrop\\2_pix4d_cleaned\"\n",
    "    project_names = [field for field in field_ids if field in os.listdir(src_folder)]   # List of projects\n",
    "    # project_names_TEST = ['E166']   # TEMP List of projects\n",
    "    geojson_file_folder = r'D:\\PhenoCrop\\3_qgis\\3_Extraction Polygons\\3. FINAL MASKS PYTHON'\n",
    "    output_root_folder = r'D:\\PhenoCrop\\3_python\\V2'\n",
    "    print(project_names)\n",
    "    # for flight_type in [\"MS\", \"3D\"]:  # Process both flight types\n",
    "    for flight_type in [\"MS\"]:  # Process only MS flight types\n",
    "        process_multiple_projects(project_names, src_folder, flight_type, geojson_file_folder, output_root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "958fb6a9-3146-41f7-a5ef-b8a7019cd82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 11:35:15,569 - INFO - Processing project: MASBASIS_2022, Flight: 20220822_T1106_MASBASIS_P4M_20m_MS_\n",
      "2025-03-10 11:35:15,571 - INFO - ⚙️ Generating NDVI Orthomosaic for: MASBASIS_2022, Flight: 20220822_T1106_MASBASIS_P4M_20m_MS_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MASBASIS_2022']\n",
      "\n",
      "🚀 Processing Project: MASBASIS_2022 | Flight Type: MS\n",
      "Fetching file lists for project: MASBASIS_2022, flight type: MS\n",
      "Warning: Orthomosaics folder not found in masbasis-P4M-20m-MS-20220616T1136\n",
      "⏳ Execution time of check_data_completeness: 0.000404 seconds\n",
      "\n",
      "✅ COMPLETENESS STATUS:\n",
      "masbasis-P4M-20m-MS-20220527T1025: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220531T1046: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220609T1207: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220614T1122: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220620T1038: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220623T1254: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220623T1333: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220629T1055: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220630T1138: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220708T1201: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220711T0933: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220714T1224: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220719T1239: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220721T1314: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220726T1034: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220728T1253: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220801T1225: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220805T1144: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220809T1059: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220812T1111: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220815T1027: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220819T1058: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220822T1106: ✅ Complete\n",
      "masbasis-P4M-20m-MS-20220825T1150: ✅ Complete\n",
      "masbasis_graminor_nobalyield-P4M-50m-MS-20220531T1112: ✅ Complete\n",
      "masbasis_graminor_nobalyield-P4M-50m-MS-20220613T1242: ✅ Complete\n",
      "masbasis_graminor_nobalyield-P4M-50m-MS-20220628T1411: ✅ Complete\n",
      "masbasis_graminor_nobalyield-P4M-50m-MS-20220805T1217: ✅ Complete\n",
      "masbasis_graminor_nobalyield-P4M-50m-MS-20220830T1134: ✅ Complete\n",
      "\n",
      "🚀 Preparing raster processing for flight: masbasis-P4M-20m-MS-20220822T1106\n",
      "✅ Matched geojson file: D:\\V-Pheno\\MASBASIS_2022-2022.geojson\n",
      "['D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_index_ndvi.tif', 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_blue.tif', 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_green.tif', 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_nir.tif', 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_red.tif', 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_red_edge.tif'] {'blue': 'reflectance_blue', 'green': 'reflectance_green', 'ndvi': 'reflectance_ndvi', 'nir': 'reflectance_nir', 'red': 'reflectance_red', 'rededge': 'reflectance_red_edge'}\n",
      "{'blue': 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_blue.tif', 'green': 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_green.tif', 'nir': 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_nir.tif', 'red': 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_red.tif', 'rededge': 'D:\\\\V-Pheno\\\\2_pix4d_cleaned\\\\MASBASIS_2022\\\\MS\\\\masbasis-P4M-20m-MS-20220822T1106\\\\2_Orthomosaics\\\\20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_red_edge.tif'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 11:35:55,629 - INFO - ✅ Successfully generated NDVI GEOTIFF\n",
      "2025-03-10 11:35:55,639 - INFO - 🚀 Preparing raster processing for flight: 20220822_T1106_MASBASIS_P4M_20m_MS_\n",
      "2025-03-10 11:35:55,640 - INFO - ⚙️ Reading shapefile: MASBASIS_2022-2022.geojson\n",
      "2025-03-10 11:35:55,764 - INFO - ⚙️ Processing NDVI Stats\n",
      "2025-03-10 11:37:38,479 - INFO - ✅ Successfully processed: NDVI Stats\n",
      "2025-03-10 11:37:38,479 - INFO - ⚙️ Processing raster: blue (20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_blue.tif)\n",
      "2025-03-10 11:40:23,488 - INFO - ✅ Successfully processed: blue\n",
      "2025-03-10 11:40:23,488 - INFO - ⚙️ Processing raster: green (20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_green.tif)\n",
      "2025-03-10 11:43:12,316 - INFO - ✅ Successfully processed: green\n",
      "2025-03-10 11:43:12,316 - INFO - ⚙️ Processing raster: nir (20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_nir.tif)\n",
      "2025-03-10 11:45:57,840 - INFO - ✅ Successfully processed: nir\n",
      "2025-03-10 11:45:57,842 - INFO - ⚙️ Processing raster: red (20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_red.tif)\n",
      "2025-03-10 11:48:47,857 - INFO - ✅ Successfully processed: red\n",
      "2025-03-10 11:48:47,857 - INFO - ⚙️ Processing raster: rededge (20220822 T1106 MASBASIS P4M 20m MS_transparent_reflectance_red_edge.tif)\n",
      "2025-03-10 11:51:30,533 - INFO - ✅ Successfully processed: rededge\n",
      "2025-03-10 11:51:30,876 - INFO - ✅📌 Statistics saved to: D:\\V-Pheno\\5_1_stats_vollebekk_python_2\\MASBASIS_2022\\20220822_T1106_MASBASIS_P4M_20m_MS_statistics.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅📌 Statistics saved to: D:\\V-Pheno\\5_1_stats_vollebekk_python_2\\MASBASIS_2022\\20220822_T1106_MASBASIS_P4M_20m_MS_statistics.csv\n",
      "⏳ Total execution time: 975.31 seconds\n",
      "🧹 Cleared raster data from memory.\n",
      "🧹 Cleared shapefile data from memory.\n",
      "📁 Saved results to D:\\V-Pheno\\5_1_stats_vollebekk_python_2\\MASBASIS_2022\\20220822_T1106_MASBASIS_P4M_20m_MS_statistics.csv\n",
      "⏳ Execution time of prepare_and_run_raster_processing: 976.039425 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_multiple_projects(project_names, src_folder, flight_type, geojson_file_folder, output_root_folder):\n",
    "    \"\"\"\n",
    "    Processes multiple projects by fetching orthomosaic files, validating data, and running raster analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - project_names (list of str): List of project names to process.\n",
    "    - src_folder (str): Root folder containing project data.\n",
    "    - flight_type (str): Either 'MS' or '3D' indicating the flight type.\n",
    "    - geojson_file_folder (str): Path to folder containing GeoJSON shapefiles.\n",
    "    - output_root_folder (str): Folder where processed CSV files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for project_name in project_names:\n",
    "        print(f\"\\n🚀 Processing Project: {project_name} | Flight Type: {flight_type}\")\n",
    "\n",
    "        # **Step 1: Generate ortho_dict**\n",
    "        ortho_dict, dsm_dtm_dict = get_project_files(src_folder, project_name, flight_type)\n",
    "\n",
    "\n",
    "\n",
    "        completeness, missing = check_data_completeness(ortho_dict, dsm_dtm_dict)\n",
    "\n",
    "        print(\"\\n✅ COMPLETENESS STATUS:\")\n",
    "        for flight, status in completeness.items():\n",
    "            print(f\"{flight}: {status}\")\n",
    "\n",
    "        if missing:\n",
    "            print(\"\\n❌ MISSING FILES REPORT:\")\n",
    "            for flight, files in missing.items():\n",
    "                print(f\"{flight} is missing: {', '.join(files)}\")\n",
    "\n",
    "        # Drop incomplete flights\n",
    "        ortho_dict, dsm_dtm_dict = drop_incomplete_flights(ortho_dict, dsm_dtm_dict, missing, output_root_folder)\n",
    "        \n",
    "        ortho_dict = {key: ortho_dict[key] for key in incomplete}\n",
    "\n",
    "        # **Step 4: Run raster processing**\n",
    "        prepare_and_run_raster_processing(project_name, output_root_folder, ortho_dict, geojson_file_folder)\n",
    "        return ortho_dict\n",
    "\n",
    "if __name__ == \"__main__\":# Example usage\n",
    "    field_ids = ['MASBASIS_2022']\n",
    "    src_folder = r\"D:\\V-Pheno\\2_pix4d_cleaned\"\n",
    "    project_names = [field for field in field_ids if field in os.listdir(src_folder)]   # List of projects\n",
    "    # project_names_TEST = ['E166']   # TEMP List of projects\n",
    "    geojson_file_folder = r'D:\\V-Pheno'\n",
    "    output_root_folder = r'D:\\V-Pheno\\5_1_stats_vollebekk_python_2'\n",
    "    print(project_names)\n",
    "    # for flight_type in [\"MS\", \"3D\"]:  # Process both flight types\n",
    "    for flight_type in [\"MS\"]:  # Process only MS flight types\n",
    "        ooro = process_multiple_projects(project_names, src_folder, flight_type, geojson_file_folder, output_root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "47f23565-3e53-474c-96ff-ced24b705416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "MS []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_multiple_projects(project_names, src_folder, flight_type, geojson_file_folder, output_root_folder):\n",
    "    \"\"\"\n",
    "    Processes multiple projects by fetching orthomosaic files, validating data, and running raster analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - project_names (list of str): List of project names to process.\n",
    "    - src_folder (str): Root folder containing project data.\n",
    "    - flight_type (str): Either 'MS' or '3D' indicating the flight type.\n",
    "    - geojson_file_folder (str): Path to folder containing GeoJSON shapefiles.\n",
    "    - output_root_folder (str): Folder where processed CSV files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for project_name in project_names:\n",
    "        print(f\"\\n🚀 Processing Project: {project_name} | Flight Type: {flight_type}\")\n",
    "\n",
    "        # **Step 1: Generate ortho_dict**\n",
    "        ortho_dict, dsm_dtm_dict = get_project_files(src_folder, project_name, flight_type)\n",
    "\n",
    "        print('here',ortho_dict)\n",
    "        completeness, missing = check_data_completeness(ortho_dict, dsm_dtm_dict)\n",
    "        \n",
    "        print(\"\\n✅ COMPLETENESS STATUS:\")\n",
    "        for flight, status in completeness.items():\n",
    "            print(f\"{flight}: {status}\")\n",
    "\n",
    "        if missing:\n",
    "            print(\"\\n❌ MISSING FILES REPORT:\")\n",
    "            for flight, files in missing.items():\n",
    "                print(f\"{flight} is missing: {', '.join(files)}\")\n",
    "\n",
    "        # Drop incomplete flights\n",
    "        ortho_dict, dsm_dtm_dict = drop_incomplete_flights(ortho_dict, dsm_dtm_dict, missing, output_root_folder)\n",
    "\n",
    "        # **Step 4: Run raster processing**\n",
    "        # prepare_and_run_raster_processing(project_name, output_root_folder, ortho_dict, geojson_file_folder)\n",
    "\n",
    "if __name__ == \"__main__\":# Example usage\n",
    "    field_ids = ['MASBASIS_2022']\n",
    "    src_folder = r\"D:\\V-Pheno\\2_pix4d_cleaned\\MASBASIS_2022\"\n",
    "    project_names = [field for field in field_ids if field in os.listdir(src_folder)]   # List of projects\n",
    "    # project_names_TEST = ['E166']   # TEMP List of projects\n",
    "    geojson_file_folder = r'D:\\V-Pheno'\n",
    "    output_root_folder = r'D:\\V-Pheno\\5_1_stats_vollebekk_python_2'\n",
    "    print(project_names)\n",
    "    # for flight_type in [\"MS\", \"3D\"]:  # Process both flight types\n",
    "    for flight_type in [\"MS\"]:  # Process only MS flight types\n",
    "        print(flight_type, project_names)\n",
    "        process_multiple_projects(project_names, src_folder, flight_type, geojson_file_folder, output_root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7598778a-6667-4991-aefd-0f5ab2a303f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete = ['masbasis-P4M-20m-MS-20220822T1106']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0510a-6542-40f4-9c5f-7e14d69f2093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4189641-87f1-4472-a632-e856b53f1233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76ab50ec-a0d8-40f9-ba03-f81aa2ab941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Usage\n",
    "# src_folder = r'D:\\PhenoCrop\\2_pix4d_cleaned\\E166\\MS\\20240812 E166 M3M 30m MS 80 85\\2_Orthomosaics'\n",
    "# paths = find_files_in_folder(src_folder, 'tif')\n",
    "# raster_files = find_raster_files(paths, RASTER_ORDER)\n",
    "# # shapefile_path = r\"D:\\PhenoCrop\\3_qgis\\3_Extraction Polygons\\3. FINAL MASKS PYTHON\\24 E 166_test.geojson\"\n",
    "# shapefile_path = r\"D:\\PhenoCrop\\3_qgis\\3_Extraction Polygons\\3. FINAL MASKS PYTHON\\24 E166_sorted_ID_corrected_coordinate_system_polygons_shrinked.geojson\"\n",
    "# project = \"E166\"\n",
    "# flight = \"20240812 E166 M3M 30m MS 80 85\"\n",
    "# output_folder = r\"D:\\PhenoCrop\\3_python\\test\"\n",
    "\n",
    "# stats_df = extract_raster_stats_multiple(shapefile_path, raster_files, project, flight, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
